{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c1efe47",
   "metadata": {},
   "source": [
    "# <font color=purple>Research Question - Use Cases</font>\n",
    "\n",
    "## <font color=purple>Background</font>\n",
    "\n",
    "Below are some research questions in demonstrating different use cases.\n",
    "\n",
    "1. **<font color=blue>What are the trends of people's occupations over the years between 1980's to 2020's?</font>**\n",
    "2. Are there more older people who work in professional roles?\n",
    "3. What is the age of people who is born outside of mainland China?\n",
    "4. Are there more younger people who was born Hong Kong than in mainland China?\n",
    "5. What are the differences in occupations between older people who were born in Taiwan and mainland China?\n",
    "6. Are there more younger people who have a university degree and who was born in Hong Kong?\n",
    "7. Does people who work in professional roles will have less children?\n",
    "\n",
    "In order to address these questions, you'll need to install and connect to a graph database instance to pull out the data to perform the analysis using machine learning and other AI approaches. Variables are based on the  volcabulary. Below are some of the variables of interest for the above research questions:\n",
    "\n",
    "- _Reference Date of Dataset_\n",
    "- _Date of Birth (derive Age)_\n",
    "- _Place of Birth_\n",
    "- _Occupation_\n",
    "- _Credential_\n",
    "- _Children_\n",
    "\n",
    "This tutorial will perform a series of data profiling, multivariate analysis, time series, etc. including using data visualisations, to address each research question. A summary of each analysis will be used to describe the relevant analysis.\n",
    "\n",
    "### Installing Python and AllegroGraph\n",
    "\n",
    "The script requires setting up the Python 3 environment. You should setup a virtual machine to ensure all Python 3 dependencies are installed and self-contained. The script also requires setting up AllegroGraph into the Python environment. [AllegraGraph](https://allegrograph.com/products/allegrograph/) is a document-centric graph database that allows researchers and AI practitioners the ability to perform SPARQL queries on heterogeneous data using _Resource Description Framework (RDF)_. More information about the development environment can be found [here](http://#).\n",
    " \n",
    "### Ingesting Data into Graph Database Instance (Local/Remote)\n",
    "\n",
    "After setting up Python and AllegroGraph environment, you'll need to import data into the graph database (AllegroGraph) instance whether this is running on your local machine or in the cloud. You can pull the necessary resources from the project's Github. A folder called `/ingestion` contains all of the JSON-LD data files. The JSON-LD files contains the relevant RDF encodings including the links to the relevant ontologies. Our system builds on the [Schema.org](https://schema.org) volcabulary. More information about the respository and graph data platform can be found [here](http://#)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd615d",
   "metadata": {},
   "source": [
    "## <font color=purple>Import Libraries</font>\n",
    "\n",
    "To begin, you'll need to import python libraries in order to be able to connect to the graph database to extract data, perform queries, perform any transformation, and to plot and analyse the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9629c83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/b/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/b/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# file processing and database\n",
    "from franz.openrdf.connect import ag_connect\n",
    "from franz.openrdf.query.query import QueryLanguage\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "from nlp_utils import *\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from wordcloud import WordCloud\n",
    "import math\n",
    "\n",
    "# ML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114ce7cc",
   "metadata": {},
   "source": [
    "## <font color=purple>SPARQL Query for Research Questions\n",
    "\n",
    "In order to develop the dataset(s) to address each of the above research questions, you need to develop several SPARQL queries. [SPARQL](https://www.w3.org/TR/rdf-sparql-query/) is a query language used for retrieving RDF data.  Datasets will depend on the research question, such as data profiling, time-series or multivariate analysis. A full list of SPARQL queries for these notes can be found [here](https://#/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0321b94b",
   "metadata": {},
   "source": [
    "### SPARQL query for Research Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string_1_1 = \"\"\"\n",
    "    PREFIX cwork: <http://schema.org/CreativeWork>\n",
    "    PREFIX per: <http://schema.org/Person>\n",
    "\n",
    "    # View triples\n",
    "    SELECT (SAMPLE(?_year) AS $year)\n",
    "           (SAMPLE(?_wave) AS $wave)\n",
    "           (COUNT(?_contributor) AS $participant_count) \n",
    "    WHERE \n",
    "    {\n",
    "      ?Dataset cwork:name $object ;\n",
    "               cwork:contentReferenceTime ?_year ;\n",
    "               cwork:identifier ?_wave ;\n",
    "               cwork:contributor ?_contributor . \n",
    "    }\n",
    "    GROUP BY ?Dataset\n",
    "    ORDER BY ?_year\n",
    "\"\"\"\n",
    "\n",
    "query_string_1_2 = \"\"\"\n",
    "    PREFIX cwork: <http://schema.org/CreativeWork>\n",
    "\n",
    "    # View triples\n",
    "    SELECT ?_contributor ?_year ?_wave\n",
    "    WHERE\n",
    "    { \n",
    "        ?Dataset cwork:name $object ;\n",
    "                 cwork:contentReferenceTime ?_year ;\n",
    "                 cwork:identifier ?_wave ;\n",
    "                 cwork:contributor ?_contributor . \n",
    "    } ORDER BY ?_year\n",
    "\"\"\"\n",
    "\n",
    "query_string_1_3 = \"\"\"\n",
    "    PREFIX per: <http://schema.org/Person>\n",
    "    PREFIX occ: <http://schema.org/Occupation>\n",
    "\n",
    "    # View triples\n",
    "    SELECT DISTINCT ?_identifier ?_occupation_name\n",
    "    WHERE\n",
    "    { \n",
    "        ?subject a ?Person ;\n",
    "                 per:identifier ?_identifier ;\n",
    "                 per:hasOccupation ?_occupation .\n",
    "        ?_occupation occ:name ?_occupation_name . \n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f5c408",
   "metadata": {},
   "source": [
    "## <font color=purple>Connect to Graph Database</font>\n",
    "\n",
    "Next you'll need to connect to the graph database instance. The current code uses a graph database instance hosted on the local machine. If the graph database instance is hosted in the cloud, then an endpoint will need to be created for consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = ag_connect('surveydatacommons', host = 'localhost', \n",
    "                  port = '10035', user = 'test', \n",
    "                  password = 'xyzzy', create=True)\n",
    "\n",
    "conn.openSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94401040",
   "metadata": {},
   "source": [
    "## <font color=purple>Retrieve Data for Analysis</font>\n",
    "\n",
    "Evaluate the results of each query and retrieve dataset for analysis. Remember to the close connection session once finished. For question 1, there are three seperate queries, the results for each query will be analysed in one dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d28a14b",
   "metadata": {},
   "source": [
    "### Data for Research Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753f7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display unaggregated data for question 1\n",
    "\n",
    "tuple_query_1_3 = conn.prepareTupleQuery(QueryLanguage.SPARQL, query_string_1_3)\n",
    "result_1_3 = tuple_query_1_3.evaluate()\n",
    "\n",
    "tuple_query_1_2 = conn.prepareTupleQuery(QueryLanguage.SPARQL, query_string_1_2)\n",
    "result_1_2 = tuple_query_1_2.evaluate()\n",
    "\n",
    "df_1_1 = pd.DataFrame({\"Year\":[], \"Wave\":[], \"Contributor\":[]})\n",
    "df_1_2 = pd.DataFrame({\"Person\":[], \"Occupation\":[]})\n",
    "\n",
    "with result_1_2:\n",
    "    for binding_set_1_2 in result_1_2:\n",
    "        y = str(binding_set_1_2.getValue(\"_year\"))\n",
    "        w = str(binding_set_1_2.getValue(\"_wave\"))\n",
    "        c = str(binding_set_1_2.getValue(\"_contributor\"))\n",
    "        contributor = c.split('#')[1].replace(\"\\\"\", \"\")\n",
    "        data = pd.DataFrame({\"Year\":y, \"Wave\":w, \"Contributor\":contributor}, index=[0])\n",
    "        df_1_1 = pd.concat([data, df_1_1.loc[:]]).reset_index(drop = True)\n",
    "result_1_2.close()\n",
    "\n",
    "with result_1_3:\n",
    "    for binding_set_1_3 in result_1_3:\n",
    "        p = str(binding_set_1_3.getValue(\"_identifier\"))\n",
    "        o = str(binding_set_1_3.getValue(\"_occupation_name\"))\n",
    "        person = p.replace(\"\\\"\", \"\")\n",
    "        data = pd.DataFrame({\"Person\":person, \"Occupation\":o}, index=[0])\n",
    "        df_1_2 = pd.concat([data, df_1_2.loc[:]]).reset_index(drop = True)\n",
    "result_1_3.close()\n",
    "\n",
    "display(df_1_1)\n",
    "display(df_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and prepare unaggregated data for question 1\n",
    "\n",
    "df_1_1.rename(columns = {\"Contributor\": \"Person\"}, inplace = True)\n",
    "df_1_2.rename(columns = {\"Person\": \"Person\"}, inplace = True)\n",
    "\n",
    "df_1_3 = pd.merge(df_1_1, df_1_2, how = \"outer\", on = [\"Person\"])\n",
    "df_1_3[\"Year\"] = df_1_3[\"Year\"].str.replace(\"\\\"\", \"\")\n",
    "df_1_3[\"Wave\"] = df_1_3[\"Wave\"].str.replace(\"\\\"\", \"\")\n",
    "df_1_3[\"Occupation\"] = df_1_3[\"Occupation\"].str.replace(\"\\\"\", \"\")\n",
    "df_1_3[\"Year\"] = pd.to_numeric(df_1_3[\"Year\"])\n",
    "df_1_3[\"Wave\"] = pd.to_numeric(df_1_3[\"Wave\"])\n",
    "df_1_3['Occupation'] = df_1_3['Occupation'].str.lower()\n",
    "df_1_3['Occupation'] = df_1_3['Occupation'].str.strip()\n",
    "\n",
    "display(df_1_3)\n",
    "display(\"Null Values: \", df_1_3[\"Occupation\"].isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad3aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display aggregated data for question 1\n",
    "\n",
    "tuple_query_1_1 = conn.prepareTupleQuery(QueryLanguage.SPARQL, query_string_1_1)\n",
    "result_1_1 = tuple_query_1_1.evaluate()\n",
    "\n",
    "df_2_2 = pd.DataFrame({\"Year\":[], \"Wave\":[], \"Person_Count\":[]})\n",
    "\n",
    "with result_1_1:\n",
    "    for binding_set_1_1 in result_1_1:\n",
    "        y = str(binding_set_1_1.getValue(\"year\"))\n",
    "        w = str(binding_set_1_1.getValue(\"wave\"))\n",
    "        pc = str(binding_set_1_1.getValue(\"participant_count\")).split('^^')[0]\n",
    "        aggre_data = pd.DataFrame({\"Year\":y, \"Wave\":w, \"Person_Count\":pc}, index=[0])\n",
    "        df_2_2 = pd.concat([aggre_data, df_2_2.loc[:]]).reset_index(drop=True)\n",
    "        \n",
    "result_1_1.close()\n",
    "conn.closeSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and prepare aggregated data for question 1\n",
    "\n",
    "df_2_2[\"Year\"] = df_2_2[\"Year\"].str.replace(\"\\\"\", \"\")\n",
    "df_2_2[\"Wave\"] = df_2_2[\"Wave\"].str.replace(\"\\\"\", \"\")\n",
    "df_2_2[\"Person_Count\"] = df_2_2[\"Person_Count\"].str.replace(\"\\\"\", \"\")\n",
    "df_2_2[\"Year\"] = pd.to_numeric(df_2_2[\"Year\"])\n",
    "df_2_2[\"Wave\"] = pd.to_numeric(df_2_2[\"Wave\"])\n",
    "df_2_2[\"Person_Count\"] = pd.to_numeric(df_2_2[\"Person_Count\"])\n",
    "df_2_1 = df_2_2.groupby(\"Year\")[\"Person_Count\"].median().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c081d68",
   "metadata": {},
   "source": [
    "## <font color=purple>Descriptive Analysis on Data<font>\n",
    "\n",
    "Cross-tables and other descriptive statistics methods can be used to analyse the data. This includes pivoting the data to fit the final analysis which can address the relevant research question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e1d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a cross table of all occupation\n",
    "\n",
    "df_occ = df_1_3[\"Occupation\"].value_counts()\n",
    "display(pd.DataFrame(df_occ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4424fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a cross table of number of people's occupation by year\n",
    "\n",
    "df_occ_year = pd.DataFrame(df_1_3.groupby(\"Year\")[\"Occupation\"].count())\n",
    "display(df_occ_year.head())\n",
    "plt.bar(df_occ_year.index, df_occ_year[\"Occupation\"], color = 'blue', width = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d900f7f",
   "metadata": {},
   "source": [
    "### Cleaning and re-coding unaggrevated occupation data  \n",
    "\n",
    "As occupational data from the graph database are extracted directly from the raw datasets, specific occupation titles are repeated, such as 'agricultural worker' and 'Agricultural worker' etc. Given the raw data in the database is directly drawn from heterogeneous sources, it's reasonable to believe that duplicates (upon manual inspection) will exist despite the transformation process. Below re-code all occupations titles that are explicitly labelled `missing`, `not applicable`, `other` or something similiar to `NA` or other erroneous by removing them from the unaggregated dataset.\n",
    "\n",
    "To reduce the complexity of the labels (and improve it's distribution), we have used unsupervised text classification to map occupational data (based on the current occupation titles which are explicitly labelled in the dataset) to the _International Standard Classification of Occupations (ISCO-08)_ codes. More information about this taxonomy can be access [here](https://isco-ilo.netlify.app/en/isco-08/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86cb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates (upon manual inspection)\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'armed forces', 'Occupation'] = 'armed force'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'directors and chief executives of enterprises', 'Occupation'] = 'directors and chief executives'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'don t know', 'Occupation'] = 'dont know'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'electrical engineering technicians', 'Occupation'] = 'electrical engineers'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'employer  manager of establishment with less than    employees', 'Occupation'] = 'employer manager of establishment with less than X employees'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'employer manager of establishment with less than    employes  less    emp', 'Occupation'] = 'employer manager of establishment with less than X employees'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'employer manager of establishment with less than    employees', 'Occupation'] = 'employer manager of establishment with less than X employees'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'missing not available', 'Occupation'] = 'missing'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'modern health ass.profess.nec', 'Occupation'] = 'modern health assessment professional'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'modern health ass.professionals', 'Occupation'] = 'modern health assessment professional'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'never had job', 'Occupation'] = 'never had a job'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'non manual   office worker   non sup', 'Occupation'] = 'non-manual office worker non-supervisor'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'non manual   office worker  non supervisor', 'Occupation'] = 'non-manual office worker non-supervisor'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'other  cs', 'Occupation'] = 'other'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'power production rel.plant operators', 'Occupation'] = 'power production plant operators'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'professional worker lawyer  accountant  teacher  etc', 'Occupation'] = 'professional worker lawyer'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'professional worker lawyer  accounta', 'Occupation'] = 'professional worker lawyer'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'supervisory   office worker   superv', 'Occupation'] = 'supervisory office worker'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'supervisory   office worker  supervises others', 'Occupation'] = 'supervisory office worker'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'unskilled manual', 'Occupation'] = 'unskilled manual worker'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'company officer', 'Occupation'] = 'officer company'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'worker agricultural', 'Occupation'] = 'agricultural worker'\n",
    "df_1_3.loc[df_1_3['Occupation'] == 'company officer', 'Occupation'] = 'officer company'\n",
    "\n",
    "# re-code 'other'\n",
    "df_1_3['Occupation'] = df_1_3['Occupation'].replace(['jp kg tj  other other'], 'other')\n",
    "\n",
    "# re-code 'missing', 'not applicable', etc.\n",
    "df_1_3['Occupation'] = df_1_3['Occupation'].replace(['missing'], np.nan)\n",
    "df_1_3['Occupation'] = df_1_3['Occupation'].replace(['skip not applicable missing'], np.nan)\n",
    "df_1_3['Occupation'] = df_1_3['Occupation'].replace(['not applicable'], np.nan)\n",
    "df_1_3['Occupation'] = df_1_3['Occupation'].replace(['dont know'], np.nan)\n",
    "df_1_3['Occupation'] = df_1_3['Occupation'].replace(['no answer'], np.nan)\n",
    "df_1_3['Occupation'] = df_1_3['Occupation'].replace(['don t want to answer'], np.nan)\n",
    "df_1_3['Occupation'] = df_1_3['Occupation'].replace(['don t know'], np.nan)\n",
    "df_1_3['Occupation'] = df_1_3['Occupation'].replace(['missing  not available'], np.nan)\n",
    "\n",
    "# output to notes and csv file\n",
    "display(df_1_3)\n",
    "df_1_3.to_csv(\"occupation_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ac9fa3",
   "metadata": {},
   "source": [
    "### Unsupervised machine learning using occupational title\n",
    "\n",
    "The next segment of these notes uses Title2Vec embeddings by [Junhua Liu et al.](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00649-5) to tag labels on occupational titles for data mining and analysis. Unsupervised text classification was used to classify occupation titles based on the  **RES**ponsibility, **FUN**ction, **LOC**ation, **O**thers scheme, and further tagging of **BIOES** scheme. This embedding has been pre-trained on occupational job titles from Asia (Singapore) and United States (Denvar). See more information [here](https://dl.acm.org/doi/10.1145/3406865.3418329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-code labels using unsupervised ML of occupation title\n",
    "df = pd.read_csv('occupation_data.csv')\n",
    "\n",
    "## clean occupation data\n",
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())    \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()    \n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]          \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]              \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "dtf = [1]\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "dtf[0] = df[\"Occupation\"].apply(lambda x: \n",
    "    utils_preprocess_text(x, flg_stemm = False, flg_lemm = True, lst_stopwords = lst_stopwords)\n",
    ")\n",
    "\n",
    "## explore occupation data using word cloud (include removing nan)\n",
    "long_string = ','.join(list(dtf[0].values))\n",
    "long_string = long_string.replace(',nan', '')\n",
    "long_string = long_string.replace(',nan,', ',')\n",
    "wordcloud = WordCloud(background_color = \"white\", \n",
    "                      max_words = 1000000, \n",
    "                      contour_width = 100, \n",
    "                      contour_color = 'steelblue',\n",
    "                      width = 1000,\n",
    "                      height = 900)\n",
    "wordcloud.generate(long_string)\n",
    "wordcloud.to_image()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd9f46",
   "metadata": {},
   "source": [
    "### Similarity-based text classification using KNN and Title2Vec as text embedding\n",
    "\n",
    "Using a similarity-based text classification approach, below uses one-hot encoding to create token and label description embeddings, before matching the text and labels using cosine similarities and KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e1402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "\n",
    "# use title2vec embeddings on occupation titles and prepare input dataset\n",
    "title2vec_embed = FlairEmbeddings(\"title2vec-lm.pt\")\n",
    "job_title = [title for title in dtf[0] if title != \"nan\" and title != \"\"]\n",
    "pd.DataFrame(pd.DataFrame(job_title).value_counts()).to_csv(\"titles_data.csv\")\n",
    "\n",
    "# get embeddings for the job titles\n",
    "df_titles = pd.read_csv(\"titles_data.csv\")\n",
    "titles = []\n",
    "for title in df_titles[\"Unnamed: 0\"].values.tolist():\n",
    "    s_jobtitle = Sentence(title)  \n",
    "    title2vec_embed.embed(s_jobtitle)\n",
    "    titles.append(s_jobtitle)\n",
    "    \n",
    "# prepare embeddings for labels using RESponsibility, FUNction, LOCation, Others scheme \n",
    "# further tagging using Beginning, Inside, Others, Ending and Single -- BIOES Scheme\n",
    "title_labels = ['RES', 'FUN', 'LOC', 'O', \n",
    "                'B-RES', 'I-RES', 'O-RES', 'E-RES', 'S-RES', \n",
    "                'B-FUN', 'I-FUN', 'O-FUN', 'E-FUN', 'S-FUN',\n",
    "                'B-LOC', 'I-LOC', 'O-LOC', 'E-LOC', 'S-LOC']\n",
    "labels_embed = []\n",
    "for label in title_labels:\n",
    "    lbl = Sentence(label)\n",
    "    title2vec_embed.embed(lbl)\n",
    "    for token in lbl:\n",
    "        embedding = token.embedding.numpy()\n",
    "        labels_embed.append(embedding)\n",
    "\n",
    "# plot number of titles verse number of tokens per title\n",
    "plt.hist([len(token) for token in titles], \n",
    "         bins = 50, \n",
    "         color = 'blue', \n",
    "         width = 0.9)\n",
    "plt.title('Token per title')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('# titles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare X, y for zero-shot learning\n",
    "X = labels_embed\n",
    "y = []\n",
    "data_ner = {\"title #\":[], \"tokens\":[], \"words\":[]}\n",
    "i = 0\n",
    "for title in titles:\n",
    "    i = i + 1\n",
    "    for token in title:\n",
    "        w_embedding = np.array(token.embedding.numpy())\n",
    "        t_embedding = token.embedding\n",
    "        y.append([i, w_embedding, t_embedding])\n",
    "        data_ner[\"title #\"].append(i)\n",
    "        data_ner[\"tokens\"].append(token)\n",
    "        data_ner[\"words\"].append(token.text)\n",
    "data_ner = pd.DataFrame.from_dict(data_ner)\n",
    "display(data_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement embedding using KNN model\n",
    "# unable to use Flair for this task as it doesn't have the right models, instead using sklearn models\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import spatial\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors = 3, radius = 0.8, metric = spatial.distance.cosine)\n",
    "neigh.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8825dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels for each job title using KNN model\n",
    "token_prediction = []\n",
    "token_prediction_distance = []\n",
    "for i in range(len(y)):\n",
    "    embed = y[i][1]\n",
    "    closest_label = neigh.kneighbors([embed], return_distance = False)[0, 0]\n",
    "    label_closest_distance = neigh.kneighbors([embed])[0][0][0]\n",
    "    tag = title_labels[closest_label]\n",
    "    token_prediction.append(tag)\n",
    "    token_prediction_distance.append(label_closest_distance)\n",
    "data_ner[\"tags\"] = token_prediction\n",
    "data_ner[\"distance\"] = token_prediction_distance\n",
    "\n",
    "display(data_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0c6a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(token_prediction).value_counts().plot(kind = \"bar\", \n",
    "                                                   figsize = (10,5), \n",
    "                                                   title = \"Title2Vec Tags in Occupation Title\",\n",
    "                                                   xlabel = \"Title2Vec Tags\",\n",
    "                                                   ylabel = \"# Tags\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9252b2b5",
   "metadata": {},
   "source": [
    "### Zero-shot learning \n",
    "\n",
    "The study by [Junhua Liu et al.](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00649-5) present results for zero-shot learning using name entity recognition evaluation using Title2Vec embedding. These are comparative baseline for the similarity approach. Results of high performing classifier using LSTM, CRF, and LSTM-CRF are shown below for name entity recognition task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb497b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_model_performance = Image.open(\"./resources/title2vec_ner_task_model.png\")\n",
    "img_tag_performance = Image.open(\"./resources/title2vec_ner_task_tags.png\")\n",
    "\n",
    "display(img_model_performance)\n",
    "display(img_tag_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2478bb8",
   "metadata": {},
   "source": [
    "### Part-of-Speech (POS) and Title2Vec clusters\n",
    "\n",
    "_**Title2Vec**_ tagging scheme:\n",
    "\n",
    "- **RES** : tokens that are associated with the level of responsibility related to the job. It can be divided into managerial level (manager, director, president etc.), operational role (technician, engineer, accountant etc.), and seniority (junior, senior, chief). \n",
    "- **FUN** : tokens that are associated with the business function within a company. It can be divided into departments (marketing, operations, finance etc.), scope of work (enterprise, national, international etc.), and content (security, education, and R&D)\n",
    "- **LOC** : tokens that are associated with geographic location of the job. It can be divided into regions (Asia, USA etc.), and Countries/States/Cities (Taiwan, Northern Taiwan, New Taipei City).  \n",
    "- **O** : tokens that are outside the other three categories\n",
    "\n",
    "\n",
    "_**BIOES**_ tagging scheme:\n",
    "\n",
    "- **B** : token that begins a span\n",
    "- **I** : tokens inside a span\n",
    "- **O** : tokens outside of any span\n",
    "- **E** : token that ends a span\n",
    "- **S** : token that gets single span\n",
    "\n",
    "\n",
    "_**ISCO-08**_ major groups:\n",
    "\n",
    "1. Managers\n",
    "2. Professionals\n",
    "3. Technicians and Associate Professionals\n",
    "4. Clerical Support Workers\n",
    "5. Service and Sales Workers\n",
    "6. Skilled Agricultural, Forestry and Fishery Workers\n",
    "7. Craft and Related Trades Workers\n",
    "8. Plant and Machine Operators, and Assemblers\n",
    "9. Elementary Occupations\n",
    "10. Armed Forces Occupations\n",
    "\n",
    "Mapping of _**ISCO-08**_ major groups and tokens clusters:\n",
    "\n",
    "| ISCO-08 | Tags | Threshold on weights (distance) |\n",
    "| :-------------------- | -----------------------------------: | :----------------------------------------------: |\n",
    "| Managers | --- | .--\n",
    "| Professionals | --- | .--\n",
    "| Technicians and Associate Professionals | --- | .--\n",
    "| Clerical Support Workers | --- | .--\n",
    "| Service and Sales Workers | --- | .--\n",
    "| Skilled Agricultural, Forestry and Fishery Workers | --- | .--\n",
    "| Craft and Related Trades Workers| --- | .--\n",
    "| Plant and Machine Operators, and Assemblers| --- | .--\n",
    "| Elementary Occupations | --- | .--\n",
    "| Armed Forces Occupations| --- | .--\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot clusters of POS and Title2Vec per token\n",
    "colors = [\"midnightblue\", \"navy\", \"royalblue\", \"cornflowerblue\", \"lightsteelblue\", \"lavender\"]\n",
    "\n",
    "# Occupational function\n",
    "\n",
    "title_lbl = [\"FUN\", \"B-FUN\", \"I-FUN\", \"O-FUN\", \"E-FUN\", \"S-FUN\"]\n",
    "i = 0\n",
    "for lbl in title_lbl:\n",
    "    cluster_tn = data_ner[data_ner[\"tags\"] == lbl]\n",
    "    cluster_tn = cluster_tn.sort_values(\"distance\")\n",
    "    if cluster_tn[\"words\"].nunique() < 60 and cluster_tn[\"words\"].nunique() > 30 :\n",
    "        plt.rcParams[\"figure.figsize\"] = (6, 15)\n",
    "    elif cluster_tn[\"words\"].nunique() < 120 and cluster_tn[\"words\"].nunique() > 90 :\n",
    "        plt.rcParams[\"figure.figsize\"] = (6, 19)\n",
    "    else:\n",
    "        plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "    plt.title(lbl + \" Tag\")\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Words')\n",
    "    plt.barh(cluster_tn[\"words\"], cluster_tn[\"distance\"], color = colors[i])\n",
    "    plt.show()\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occupational responsibilities\n",
    "\n",
    "title_lbl = [\"RES\", 'B-RES', 'I-RES', 'O-RES', 'E-RES', 'S-RES',]\n",
    "i = 0\n",
    "for lbl in title_lbl:\n",
    "    cluster_tn = data_ner[data_ner[\"tags\"] == lbl]\n",
    "    cluster_tn = cluster_tn.sort_values(\"distance\")\n",
    "    if cluster_tn[\"words\"].nunique() < 70 and cluster_tn[\"words\"].nunique() > 30 :\n",
    "        plt.rcParams[\"figure.figsize\"] = (6, 15)\n",
    "    elif cluster_tn[\"words\"].nunique() < 150 and cluster_tn[\"words\"].nunique() > 80 :\n",
    "        plt.rcParams[\"figure.figsize\"] = (6, 30)\n",
    "    elif cluster_tn[\"words\"].nunique() < 20 and cluster_tn[\"words\"].nunique() > 10 :\n",
    "        plt.rcParams[\"figure.figsize\"] = (6, 8)\n",
    "    else:\n",
    "        plt.rcParams[\"figure.figsize\"] = (6, 1)\n",
    "    plt.title(lbl + \" Tag\")\n",
    "    plt.xlabel('Weights')\n",
    "    plt.ylabel('Words')\n",
    "    plt.barh(cluster_tn[\"words\"], cluster_tn[\"distance\"], color = colors[i])\n",
    "    plt.show()\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ace75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occupational outside categories (other)\n",
    "\n",
    "title_lbl = [\"O\"]\n",
    "i = 0\n",
    "for lbl in title_lbl:\n",
    "    cluster_tn = data_ner[data_ner[\"tags\"] == lbl]\n",
    "    cluster_tn = cluster_tn.sort_values(\"distance\")\n",
    "    if cluster_tn[\"words\"].nunique() < 70 and cluster_tn[\"words\"].nunique() > 30 :\n",
    "        plt.rcParams[\"figure.figsize\"] = (6, 15)\n",
    "    elif cluster_tn[\"words\"].nunique() < 150 and cluster_tn[\"words\"].nunique() > 80 :\n",
    "        plt.rcParams[\"figure.figsize\"] = (6, 20)\n",
    "    elif cluster_tn[\"words\"].nunique() < 20 and cluster_tn[\"words\"].nunique() > 10 :\n",
    "        plt.rcParams[\"figure.figsize\"] = (6, 8)\n",
    "    else:\n",
    "        plt.rcParams[\"figure.figsize\"] = (6, 1)\n",
    "    plt.title(lbl + \" Tag\")\n",
    "    plt.xlabel('Weights')\n",
    "    plt.ylabel('Words')\n",
    "    plt.barh(cluster_tn[\"words\"], cluster_tn[\"distance\"], color = colors[i])\n",
    "    plt.show()\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e8d46",
   "metadata": {},
   "source": [
    "## <font color=purple>Plot trends over time</font>\n",
    "\n",
    "A time series line plot can be used to examine the changing characteristics of people over time, such as different occupations, the number of children, and types of education."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e41e24",
   "metadata": {},
   "source": [
    "### Analyse the trends in the number of participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c1a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_2_1.head())\n",
    "\n",
    "plt.bar(df_2_1.index, \n",
    "        df_2_1[\"Person_Count\"], \n",
    "        color = 'blue', \n",
    "        width = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624cd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_2_2)\n",
    "\n",
    "plt.plot(df_2_2[\"Year\"], df_2_2[\"Person_Count\"], \n",
    "         marker = 'o', \n",
    "         markerfacecolor = 'yellow', \n",
    "         markeredgecolor = 'red', \n",
    "         markersize = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76475358",
   "metadata": {},
   "source": [
    "### Analyse the trends in participant's occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eaa94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec4083c4",
   "metadata": {},
   "source": [
    "## <font color=purple>Interpretation of Results</font>\n",
    "\n",
    "As we examine the plots, you can interpret the results of the analysis. Below provides a brief summary of the result addressing each of the research questions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
